context_model : 'GPT' #one of 'RNN', 'LSTM', 'GPT'
model_input_size:  [768]
model_output_size: [768]
num_layers: [2]
max_vocab_size: 50000 #Gulordava setup
nheads : [16]  #for GPT only
ffn_hidden : [2048]   #for GPT only
tie_weights : True
tie_layers : False
dropout: [0.0]
epochs: [1]
batch_size: [1]
bptt_chunk : [150]     #size of context for truncated BPTT
learning_rate: [0.002]
warmup_epochs: [1]       #number of epochs for warmup
warmup_batch_size : [8]  #size of batches during warmup
restart_cycles: [1]      #number or warmup restarts for GPT only
positional : True        #use positional embeddings or ignore them

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f084609",
   "metadata": {},
   "source": [
    "Evaluation of language models on agreement task\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcfe5e5",
   "metadata": {},
   "source": [
    "This notebook provides methodological guidelines for testing the performance of pre-trained language models on agreeement task .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca94608",
   "metadata": {},
   "source": [
    "Loading a pretrained transformer language model\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8391fc",
   "metadata": {},
   "source": [
    "The best pretrained model for each architecture are stored in their `MODEL_DIR` named `tied_layers_30_7` (paramètres partagés) and `no_tied_layers_37_9`, the numbers 30.7/37.9 indicate the perplexity of LM.\n",
    "- `lm_params.pt`: the pretrained model to load\n",
    "- `model.yaml`: values of the hyperparameters\n",
    "- `tokcodes`: vocabulary of language model (50k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc9e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnlm import load_transformer_model\n",
    "from data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f650a",
   "metadata": {},
   "source": [
    "Incremental Language models\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa7518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use positional embeddings\n"
     ]
    }
   ],
   "source": [
    "# load models with shared parameters (tied_layers)\n",
    "model_dir = \"tied_layers_30_7/\"\n",
    "encoder, lm = load_transformer_model(model_dir,cpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a6939",
   "metadata": {},
   "source": [
    "**Next word prediction** Incremental language models perform naturally the task of next word prediction. Given a batch of sentences, we can predict the next word log probability using the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60030fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <bos> Les offres que le directeur a acceptées sont intéressantes\n",
      "           token       ref_next     pred_next  ref_prob  pred_prob\n",
      "0          <bos>            Les            Le -2.889215  -2.234221\n",
      "1            Les         offres         <unk> -9.581299  -3.058100\n",
      "2         offres            que            de -6.800640  -1.432299\n",
      "3            que             le           les -2.670989  -1.939359\n",
      "4             le      directeur  gouvernement -6.045998  -2.295597\n",
      "5      directeur              a            de -2.008267  -1.628106\n",
      "6              a      acceptées         <unk> -6.883583  -2.724935\n",
      "7      acceptées           sont          sont -1.640829  -1.640829\n",
      "8           sont  intéressantes            en -9.809179  -3.100528\n",
      "9  intéressantes              .          pour -1.502111  -1.251670\n",
      "\n",
      "input: <bos> Les offres que le directeur a acceptée sont intéressantes\n",
      "           token       ref_next     pred_next  ref_prob  pred_prob\n",
      "0          <bos>            Les            Le -2.889215  -2.234221\n",
      "1            Les         offres         <unk> -9.581299  -3.058100\n",
      "2         offres            que            de -6.800640  -1.432299\n",
      "3            que             le           les -2.670989  -1.939359\n",
      "4             le      directeur  gouvernement -6.045998  -2.295597\n",
      "5      directeur              a            de -2.008267  -1.628106\n",
      "6              a       acceptée         <unk> -7.570808  -2.724935\n",
      "7       acceptée           sont          sont -1.990716  -1.990716\n",
      "8           sont  intéressantes            en -9.684201  -3.007268\n",
      "9  intéressantes              .          pour -1.468075  -1.241169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "testdata = Dataset(\"examples_obj_pp.txt\", parentencoding=model_dir)\n",
    "for elt in lm.predict(testdata,batch_size,device='cpu'):\n",
    "    print('input:',' '.join(elt['token']))\n",
    "    print(elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc07871",
   "metadata": {},
   "source": [
    "We compare, given a `prefix` (e.g. Les offres que le directeur a `____` ), the probabilities a language model assigns to the plural form of the target participle (`acceptées`) and its singular form (`acceptée`). We consider the model has predicted the agreement correctly if the form with the correct number has a higer probability:\n",
    "\n",
    "In the above example:\n",
    "\n",
    "p(`acceptées` | the prefix) = -6.883583\n",
    "\n",
    "p(`acceptée`  | the prefix) = -7.570808\n",
    "\n",
    "p(`acceptées` | the prefix)> p(`acceptée` | the prefix) $\\rightarrow$ The transformer model predict the plural form. \n",
    "\n",
    "### evaluation data(test.txt) format\n",
    "\n",
    "For each prefix/sentence, we have the right form `Les offres que le directeur a acceptées` and the wrong form `Les offres que le directeur a acceptée`. This format allows us to compare more easily the predictions of a language model given the same prefix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00bdb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
